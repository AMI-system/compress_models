{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0fa305-dc01-4842-864b-a1e397bcdf54",
   "metadata": {},
   "source": [
    "# Script to compress existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e1ddf-8565-4449-8d2c-5b15b3400b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from PIL import Image\n",
    "import onnx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae49ac6-261a-4c71-825f-25f455221005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the files you are interested in\n",
    "image_file='/bask/homes/f/fspo1218/amber/data/gbif_download_standalone/gbif_images/Noctuidae/Spodoptera/Spodoptera exigua/1211977745.jpg'\n",
    "\n",
    "region = 'costarica'\n",
    "\n",
    "# Label info for the species names for the uk macro moths\n",
    "f = open(f\"/bask/homes/f/fspo1218/amber/data/gbif_{region}/02_{region}_data_numeric_labels.json\")\n",
    "label_info = json.load(f)\n",
    "label_info = label_info['species_list']\n",
    "species_list_mila = list(label_info)\n",
    "print(len(species_list_mila), \" species in total\")\n",
    "\n",
    "num_classes = len(species_list_mila)\n",
    "\n",
    "\n",
    "files = os.listdir(\"/bask/homes/f/fspo1218/amber/projects/on_device_classifier/outputs/\")\n",
    "PATH = os.path.join(\"/bask/homes/f/fspo1218/amber/projects/on_device_classifier/outputs/\",\n",
    "               [file for file in files if region in file and 'resnet50' in file and 'state' not in file][1])\n",
    "print(PATH)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "output_dir = f'/bask/homes/f/fspo1218/amber/data/compressed_models/gbif_{region}/'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bbd4d-53f2-48c1-a2b6-6730089f8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_to_tflite(model, output_dir, image, output_model_prefix=\"model\"):\n",
    "\n",
    "    # convert the model to onnx\n",
    "    print(\"Converting to onnx\")\n",
    "\n",
    "    onnx_path = output_dir + \"/\" + output_model_prefix + \".onnx\"\n",
    "    torch.onnx.export(\n",
    "            model=model.eval(),\n",
    "            args=image.unsqueeze(0),\n",
    "            f=onnx_path,\n",
    "            verbose=False,\n",
    "            export_params=True,\n",
    "            do_constant_folding=False,\n",
    "            input_names=['input'],\n",
    "            opset_version=12,\n",
    "            output_names=['output']\n",
    "    )\n",
    "\n",
    "    # Convert to tf\n",
    "    print(\"Converting to tensorflow...\")\n",
    "    tf_path = output_dir + \"/tf_\" + output_model_prefix\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    tf_rep = prepare(onnx_model, device='CPU')\n",
    "    tf_rep.export_graph(tf_path)\n",
    "\n",
    "    # Convert to tfLite\n",
    "    print(\"Converting to tensorflowlite\")\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
    "    converter.experimental_new_converter = True\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter.allow_custom_ops=True\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    print(\"Saving converted model\")\n",
    "    with open(output_dir + \"/\" + output_model_prefix + \".tflite\", 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab3099-3671-49af-9336-afa2a99e01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_file)\n",
    "\n",
    "# Transform\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((300, 300)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "img = transform(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9c55e-7b8f-4b73-baaf-f8c5820ed0c1",
   "metadata": {},
   "source": [
    "## MILA species classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d75d86-b5fe-4c6e-be85-df3e9f4cba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append('/bask/homes/f/fspo1218/amber/projects/species_classifier/')\n",
    "sys.path.append('/bask/homes/f/fspo1218/amber/projects/species_classifier/models/')\n",
    "sys.path.append('/bask/homes/f/fspo1218/amber/projects/species_classifier/data2/')\n",
    "sys.path.append('/bask/homes/f/fspo1218/amber/projects/species_classifier/evaluation/')\n",
    "\n",
    "from data2 import dataloader\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c4ada-c1fe-49e9-abb8-0b2be0fc9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'efficientnet' in PATH:\n",
    "    model_py_mila = models.efficientnet_b0(pretrained=True)\n",
    "    model_py_mila = model_py_mila.to(device)\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    model_py_mila.eval()\n",
    "\n",
    "elif 'resnet' in PATH:\n",
    "    model_py_mila = torchvision.models.resnet50(weights=None)\n",
    "    num_ftrs = model_py_mila.fc.in_features\n",
    "    model_py_mila.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "    model_py_mila = model_py_mila.to(device)\n",
    "    model_py_mila = torch.load(PATH, map_location=device)\n",
    "    model_py_mila.eval()\n",
    "\n",
    "else:\n",
    "    print('clarify model type')\n",
    "\n",
    "print(\"loaded MILA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f765865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state_dict\n",
    "#torch.save(model_py_mila.state_dict(), PATH.replace('resnet50', 'state_resnet50'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d7017-855f-40e2-81c7-0bd7d468e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = 'resnet_' + region\n",
    "\n",
    "tflite_model = pytorch_to_tflite(model_py_mila,\n",
    "                  output_dir=output_dir,\n",
    "                  image=img,\n",
    "                  output_model_prefix=pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52610fd3-49fb-41d9-bbf8-c7614bf6e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getsize(PATH) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6f419",
   "metadata": {},
   "source": [
    "# Localisation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the localizmodel\n",
    "weights_path = \"/bask/homes/f/fspo1218/amber/data/mila_models/v1_localizmodel_2021-08-17-12-06.pt\"\n",
    "\n",
    "model_loc = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "num_classes = 2  # 1 class (object) + background\n",
    "in_features = model_loc.roi_heads.box_predictor.cls_score.in_features\n",
    "model_loc.roi_heads.box_predictor = (\n",
    "    torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "state_dict = checkpoint.get(\"model_state_dict\") or checkpoint\n",
    "model_loc.load_state_dict(state_dict)\n",
    "model_loc = model_loc.to(device)\n",
    "#model_loc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc49bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir2 = output_dir.split('gbif_', 1)[0]\n",
    "\n",
    "print(output_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608bdf0-2e84-44f7-84d3-bb32543f935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import onnx\n",
    "from onnx_tf.backend_rep import TensorflowRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = 'localisation_' + region\n",
    "\n",
    "model = model_loc\n",
    "image=img\n",
    "output_model_prefix=pref\n",
    "\n",
    "# convert the model to onnx\n",
    "print(\"Converting to onnx\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Example input tensor (replace this with your own input tensor)\n",
    "input_tensor = img.unsqueeze(0)\n",
    "\n",
    "# Display the shape of the input tensor\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "\n",
    "# # Export the PyTorch model to ONNX format\n",
    "onnx_path = output_dir2 + \"faster_rcnn.onnx\"\n",
    "torch.onnx.export(model, input_tensor, onnx_path, verbose=True)\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# # Prepare the ONNX model for conversion to TensorFlow\n",
    "# tf_rep = prepare(onnx_model)\n",
    "\n",
    "# # Convert the ONNX model to TensorFlow Lite\n",
    "# tflite_path = \"faster_rcnn.tflite\"\n",
    "# converter = tf.lite.TFLiteConverter.from_concrete_functions([tf_rep.graph.as_graph_def(add_shapes=True)])\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the TFLite model to a file\n",
    "# with open(tflite_path, 'wb') as f:\n",
    "#     f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef901133-1c93-4d00-87de-c667484660b6",
   "metadata": {},
   "source": [
    "### Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26455a38-7355-447a-bda8-c3406e627648",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "axs.imshow(img.permute(1, 2, 0))\n",
    "axs.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce3fb1-8701-4384-8585-2822f4feeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def pytorch_inference(image, test_model, print_time=False):\n",
    "    a = datetime.datetime.now()\n",
    "    output = test_model(image.unsqueeze(0))\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "    predictions = predictions.detach().numpy()\n",
    "\n",
    "    categories = predictions.argmax(axis=1)\n",
    "    #print(categories)\n",
    "    b = datetime.datetime.now()\n",
    "    c = b - a\n",
    "    if print_time: print(str(c.microseconds) + \"\\u03bcs\")\n",
    "    return(categories[0])\n",
    "\n",
    "def tflite_inference(image, interpreter, print_time=False):\n",
    "    a = datetime.datetime.now()\n",
    "    interpreter.set_tensor(input_details[0]['index'], image.unsqueeze(0))\n",
    "    interpreter.invoke()\n",
    "    outputs_tf = interpreter.get_tensor(output_details[0]['index'])\n",
    "    prediction_tf = np.squeeze(outputs_tf)\n",
    "    prediction_tf = prediction_tf.argsort()[::-1][0]\n",
    "    #print(prediction_tf)\n",
    "    b = datetime.datetime.now()\n",
    "    c = b - a\n",
    "    if print_time: print(str(c.microseconds) + \"\\u03bcs\")\n",
    "    return(prediction_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146030a2-c420-42bc-9b91-ba90a03ea354",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_inf = pytorch_inference(img, model_py_mila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc871-6067-486b-83fd-d6267bcff4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "pref2 = pref + '.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(output_dir, pref2))\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6f642-24b3-4637-876b-ca86b7d070e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_inf = tflite_inference(img, interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d271c-829e-4126-9fc0-27331d530dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFlite says', species_list_mila[tflite_inf])\n",
    "print('Pytorch says', species_list_mila[pytorch_inf])\n",
    "print('Truth says', os.path.basename(os.path.dirname(image_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbf4cc-f472-4c5e-9bcf-fea37df635d3",
   "metadata": {},
   "source": [
    "# Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ecdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = f'/bask/homes/f/fspo1218/amber/projects/on_device_classifier/configs/01_{region}_data_config.json'\n",
    "f = open(config_file)\n",
    "config_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = os.listdir(f'/bask/homes/f/fspo1218/amber/data/gbif_{region}/test/')\n",
    "\n",
    "# subset to only those of format 'test-*.tar'\n",
    "len_test = [file for file in len_test if 'test-' in file and '.tar' in file]\n",
    "len_test = len(len_test) - 1\n",
    "\n",
    "# padd the number to 6 digits\n",
    "len_test = str(len_test).zfill(6)\n",
    "\n",
    "print(len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b981bd3-460f-4b85-89f0-88e246c27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resize = config_data[\"training\"][\"image_resize\"]\n",
    "batch_size = config_data[\"training\"][\"batch_size\"]\n",
    "label_list = config_data[\"dataset\"][\"label_info\"]\n",
    "preprocess_mode = config_data[\"model\"][\"preprocess_mode\"]\n",
    "taxon_hierar = config_data[\"dataset\"][\"taxon_hierarchy\"]\n",
    "label_info= config_data[\"dataset\"][\"label_info\"]\n",
    "\n",
    "pass_str = '/bask/homes/f/fspo1218/amber/data/gbif_' + region + '/test/test-500-{000000..' + len_test + '}.tar'\n",
    "\n",
    "# Load in the test data\n",
    "test_dataloader = dataloader.build_webdataset_pipeline(\n",
    "        sharedurl=pass_str,\n",
    "        input_size=image_resize,\n",
    "        batch_size=batch_size,\n",
    "        is_training=False,\n",
    "        num_workers=4,\n",
    "        preprocess_mode=preprocess_mode,\n",
    "    )\n",
    "print(\"images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc717ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(output_dir, pref2))\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preprocess_for_tflite and postprocess_for_tflite functions accordingly\n",
    "def preprocess_for_tflite(image_batch):\n",
    "    return np.array(image_batch)\n",
    "\n",
    "def postprocess_for_tflite(output_data):\n",
    "    return torch.tensor(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import micro_accuracy_batch\n",
    "from evaluation import macro_accuracy_batch\n",
    "from evaluation import confusion_data_conversion\n",
    "from evaluation import confusion_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_iterations = sum(1 for _ in enumerate(test_dataloader))\n",
    "\n",
    "print('There are ', no_iterations, ' iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b77575",
   "metadata": {},
   "source": [
    "PSA: the next cell takes around 20 mins for 94 batches (64 images each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb507c-e94d-4965-b3be-f14860fe57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_global_microacc_data = None\n",
    "py_global_macroacc_data = None\n",
    "py_global_confusion_data_sp = None\n",
    "py_global_confusion_data_g = None\n",
    "py_global_confusion_data_f = None\n",
    "\n",
    "tf_global_microacc_data = None\n",
    "tf_global_macroacc_data = None\n",
    "tf_global_confusion_data_sp = None\n",
    "tf_global_confusion_data_g = None\n",
    "tf_global_confusion_data_f = None\n",
    "i=1\n",
    "\n",
    "for image_batch, label_batch in test_dataloader:\n",
    "    print(i, '/', no_iterations)\n",
    "    i = i + 1\n",
    "\n",
    "    image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "    py_predictions = model_py_mila(image_batch)\n",
    "\n",
    "    # Preprocess the input image_batch for TensorFlow Lite model\n",
    "    # You need to replace this preprocessing logic based on your TensorFlow Lite model requirements\n",
    "    input_data = preprocess_for_tflite(image_batch)\n",
    "\n",
    "    # Run inference using TensorFlow Lite model for each image in the batch\n",
    "    predictions_tflite_batch = []\n",
    "    for single_input_data in input_data:\n",
    "        single_input_data = np.expand_dims(single_input_data, axis=0)  # Add batch dimension\n",
    "        interpreter.set_tensor(interpreter.get_input_details()[0]['index'], single_input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
    "        predictions_tflite_batch.append(output_data)\n",
    "\n",
    "    # Stack predictions for the entire batch\n",
    "    predictions_tflite_batch = np.vstack(predictions_tflite_batch)\n",
    "\n",
    "    # Assuming `postprocess_for_tflite` is a function to postprocess the output_data\n",
    "    # You need to replace this postprocessing logic based on your TensorFlow Lite model requirements\n",
    "    tf_predictions = postprocess_for_tflite(predictions_tflite_batch)\n",
    "\n",
    "    #predictions_tf = predictions_tflite_batch#.argmax(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Pytorch Metrics\n",
    "    # micro-accuracy calculation\n",
    "    py_micro_accuracy = micro_accuracy_batch.MicroAccuracyBatch(\n",
    "        py_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).batch_accuracy()\n",
    "    py_global_microacc_data = micro_accuracy_batch.add_batch_microacc(\n",
    "        py_global_microacc_data, py_micro_accuracy\n",
    "    )\n",
    "    # macro-accuracy calculation\n",
    "    py_macro_accuracy = macro_accuracy_batch.MacroAccuracyBatch(\n",
    "        py_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).batch_accuracy()\n",
    "    py_global_macroacc_data = macro_accuracy_batch.add_batch_macroacc(\n",
    "        py_global_macroacc_data, py_macro_accuracy\n",
    "    )\n",
    "\n",
    "    # confusion matrix\n",
    "    (\n",
    "        py_sp_label_batch,\n",
    "        py_sp_predictions,\n",
    "        py_g_label_batch,\n",
    "        py_g_predictions,\n",
    "        py_f_label_batch,\n",
    "        py_f_predictions,\n",
    "    ) = confusion_data_conversion.ConfusionDataConvert(\n",
    "        py_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).converted_data()\n",
    "\n",
    "    py_global_confusion_data_sp = confusion_matrix_data.confusion_matrix_data(\n",
    "        py_global_confusion_data_sp, [py_sp_label_batch, py_sp_predictions]\n",
    "    )\n",
    "    py_global_confusion_data_g = confusion_matrix_data.confusion_matrix_data(\n",
    "        py_global_confusion_data_g, [py_g_label_batch, py_g_predictions]\n",
    "    )\n",
    "    py_global_confusion_data_f = confusion_matrix_data.confusion_matrix_data(\n",
    "        py_global_confusion_data_f, [py_f_label_batch, py_f_predictions]\n",
    "    )\n",
    "\n",
    "    # TFLite Metrics\n",
    "    # micro-accuracy calculation\n",
    "    tf_micro_accuracy = micro_accuracy_batch.MicroAccuracyBatch(\n",
    "        tf_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).batch_accuracy()\n",
    "    tf_global_microacc_data = micro_accuracy_batch.add_batch_microacc(\n",
    "        tf_global_microacc_data, tf_micro_accuracy\n",
    "    )\n",
    "    # macro-accuracy calculation\n",
    "    tf_macro_accuracy = macro_accuracy_batch.MacroAccuracyBatch(\n",
    "        tf_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).batch_accuracy()\n",
    "    tf_global_macroacc_data = macro_accuracy_batch.add_batch_macroacc(\n",
    "        tf_global_macroacc_data, tf_macro_accuracy\n",
    "    )\n",
    "\n",
    "    # confusion matrix\n",
    "    (\n",
    "        tf_sp_label_batch,\n",
    "        tf_sp_predictions,\n",
    "        tf_g_label_batch,\n",
    "        tf_g_predictions,\n",
    "        tf_f_label_batch,\n",
    "        tf_f_predictions,\n",
    "    ) = confusion_data_conversion.ConfusionDataConvert(\n",
    "        tf_predictions, label_batch, label_info, taxon_hierar\n",
    "    ).converted_data()\n",
    "\n",
    "    tf_global_confusion_data_sp = confusion_matrix_data.confusion_matrix_data(\n",
    "        tf_global_confusion_data_sp, [tf_sp_label_batch, tf_sp_predictions]\n",
    "    )\n",
    "    tf_global_confusion_data_g = confusion_matrix_data.confusion_matrix_data(\n",
    "        tf_global_confusion_data_g, [tf_g_label_batch, tf_g_predictions]\n",
    "    )\n",
    "    tf_global_confusion_data_f = confusion_matrix_data.confusion_matrix_data(\n",
    "        tf_global_confusion_data_f, [tf_f_label_batch, tf_f_predictions]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_global_confusion_data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_read = json.load(open(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118d6d8-7c52-4e8a-8199-5a3da31b902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "py_final_micro_accuracy = micro_accuracy_batch.final_microacc(py_global_microacc_data)\n",
    "py_final_macro_accuracy, py_taxon_acc = macro_accuracy_batch.final_macroacc(py_global_macroacc_data)\n",
    "\n",
    "tf_final_micro_accuracy = micro_accuracy_batch.final_microacc(tf_global_microacc_data)\n",
    "tf_final_macro_accuracy, tf_taxon_acc = macro_accuracy_batch.final_macroacc(tf_global_macroacc_data)\n",
    "\n",
    "tf_tax_accuracy = macro_accuracy_batch.taxon_accuracy(tf_taxon_acc, label_read)\n",
    "py_tax_accuracy = macro_accuracy_batch.taxon_accuracy(py_taxon_acc, label_read)\n",
    "\n",
    "print(py_final_micro_accuracy, py_final_macro_accuracy)\n",
    "print(tf_final_micro_accuracy, tf_final_macro_accuracy)\n",
    "\n",
    "# saving evaluation data to file\n",
    "confdata_pd_f = pd.DataFrame(\n",
    "    {\n",
    "        \"F_Truth\": py_global_confusion_data_f[0].reshape(-1),\n",
    "        \"F_Py_Prediction\": py_global_confusion_data_f[1].reshape(-1),\n",
    "        \"F_Tf_Prediction\": tf_global_confusion_data_f[1].reshape(-1),\n",
    "    }\n",
    ")\n",
    "confdata_pd_g = pd.DataFrame(\n",
    "    {\n",
    "        \"G_Truth\": py_global_confusion_data_g[0].reshape(-1),\n",
    "        \"G_Py_Prediction\": py_global_confusion_data_g[1].reshape(-1),\n",
    "        \"G_Tf_Prediction\": tf_global_confusion_data_g[1].reshape(-1),\n",
    "    }\n",
    ")\n",
    "confdata_pd_sp = pd.DataFrame(\n",
    "    {\n",
    "        \"S_Truth\": py_global_confusion_data_sp[0].reshape(-1),\n",
    "        \"S_Py_Prediction\": py_global_confusion_data_sp[1].reshape(-1),\n",
    "        \"S_Tf_Prediction\": tf_global_confusion_data_sp[1].reshape(-1),\n",
    "    }\n",
    ")\n",
    "confdata_pd = pd.concat([confdata_pd_f, confdata_pd_g, confdata_pd_sp], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outputs\n",
    "\n",
    "confdata_pd.to_csv('./outputs/' + region + '_resnet' + \"_v2.0\" + \"_confusion-data.csv\", index=False)\n",
    "\n",
    "with open(\n",
    "    './outputs/' + region + '_resnet' + \"_v2.0\" + \"_micro-accuracy.json\", \"w\"\n",
    ") as outfile:\n",
    "   json.dump( {'Pytorch': py_final_micro_accuracy, 'TFLite': tf_final_micro_accuracy}, outfile)\n",
    "\n",
    "with open(\n",
    "    './outputs/' + region + '_resnet' + \"_v2.0\" + \"_macro-accuracy.json\", \"w\"\n",
    ") as outfile:\n",
    "   json.dump( {'Pytorch': py_final_macro_accuracy, 'TFLite': tf_final_macro_accuracy}, outfile)\n",
    "\n",
    "with open(\n",
    "    './outputs/' + region + '_resnet' + \"_v2.0\" + \"_taxon-accuracy.json\", \"w\"\n",
    ") as outfile:\n",
    "    json.dump( {'Pytorch': py_tax_accuracy, 'TFLite': tf_tax_accuracy}, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e6329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_conda_env2 (Conda)",
   "language": "python",
   "name": "sys_kg_conda_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
